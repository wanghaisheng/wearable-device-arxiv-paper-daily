
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../../smart%20ring/smart%20ring/">
      
      
        <link rel="next" href="../../wearable%20device/wearable%20device/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.1, mkdocs-material-9.1.21">
    
    
      
        <title>Smart watch - wearable-device-paper-daily</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.eebd395e.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
<script type="text/javascript">
    (function(c,l,a,r,i,t,y){
        c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
        t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
        y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
    })(window, document, "clarity", "script", "hh1oiyc7g7");
</script>

    
    
  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#smart-watch" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="wearable-device-paper-daily" class="md-header__button md-logo" aria-label="wearable-device-paper-daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            wearable-device-paper-daily
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Smart watch
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/wanghaisheng/wearable-device-arxiv-paper-daily" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="wearable-device-paper-daily" class="md-nav__button md-logo" aria-label="wearable-device-paper-daily" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    wearable-device-paper-daily
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/wanghaisheng/wearable-device-arxiv-paper-daily" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        arxiv-daily latest papers around wearable device arxiv paper daily
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Electroencephalography
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Electroencephalography
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Electroencephalography/Electroencephalography/" class="md-nav__link">
        Electroencephalography
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          Electromyography
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Electromyography
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Electromyography/Electromyography/" class="md-nav__link">
        Electromyography
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          PPG
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          PPG
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../PPG/Photoplethysmography/" class="md-nav__link">
        Photoplethysmography
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          Actigraphy
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Actigraphy
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../actigraphy/actigraphy/" class="md-nav__link">
        Actigraphy
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
          All search terms
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          All search terms
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../all%20search%20terms/all%20search%20terms/" class="md-nav__link">
        All search terms
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
          Apple
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Apple
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../apple/apple%20watch/" class="md-nav__link">
        Apple watch
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
          Camera
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          Camera
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../camera/wearable%20camera/" class="md-nav__link">
        Wearable camera
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
          Heart rate
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_9">
          <span class="md-nav__icon md-icon"></span>
          Heart rate
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../heart%20rate/heart%20rate/" class="md-nav__link">
        Heart rate
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10" >
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_10" id="__nav_10_label" tabindex="0">
          Huawei
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_10">
          <span class="md-nav__icon md-icon"></span>
          Huawei
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../huawei/huawei%20band/" class="md-nav__link">
        Huawei band
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../huawei/huawei%20watch/" class="md-nav__link">
        Huawei watch
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_11" id="__nav_11_label" tabindex="0">
          Smart glass
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_11_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_11">
          <span class="md-nav__icon md-icon"></span>
          Smart glass
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../smart%20glass/smart%20glass/" class="md-nav__link">
        Smart glass
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_12" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_12" id="__nav_12_label" tabindex="0">
          Smart ring
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_12_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_12">
          <span class="md-nav__icon md-icon"></span>
          Smart ring
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../smart%20ring/smart%20ring/" class="md-nav__link">
        Smart ring
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_13" checked>
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_13" id="__nav_13_label" tabindex="0">
          Smart watch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_13_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_13">
          <span class="md-nav__icon md-icon"></span>
          Smart watch
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Smart watch
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Smart watch
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#smart-watch" class="md-nav__link">
    smart watch
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_14" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_14" id="__nav_14_label" tabindex="0">
          Wearable device
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_14_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_14">
          <span class="md-nav__icon md-icon"></span>
          Wearable device
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../wearable%20device/wearable%20device/" class="md-nav__link">
        Wearable device
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#smart-watch" class="md-nav__link">
    smart watch
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Smart watch</h1>

<h3 id="smart-watch">smart watch</h3>
<table>
<thead>
<tr>
<th style="text-align: center;">Publish Date</th>
<th style="text-align: center;">Title</th>
<th style="text-align: center;">Authors</th>
<th style="text-align: center;">PDF</th>
<th style="text-align: center;">Code</th>
<th style="text-align: center;">Abstract</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><strong>2023-07-27</strong></td>
<td style="text-align: center;"><strong>Cavity-Mediated Molecular Entanglement and Generation of Non-Classical States of Light</strong></td>
<td style="text-align: center;">Davis M. Welakuh et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.15047v1">2307.15047v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">The generation and control of entanglement in a quantum mechanical system is a critical element of nearly all quantum applications. Molecular systems are a promising candidate, with numerous degrees of freedom able to be targeted. However, knowledge of inter-system entanglement mechanisms in such systems is limited. In this work, we demonstrate the generation of entanglement between vibrational degrees of freedom in molecules via strong coupling to a cavity mode driven by a weak coherent field. In a bi-molecular system, we show entanglement can not only be generated between the cavity and molecular system, but also between molecules. This process also results in the generation of non-classical states of light, providing potential pathways for harnessing entanglement in molecular systems.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-27</strong></td>
<td style="text-align: center;"><strong>Smart Contract Migration: Security Analysis and Recommendations from Ethereum to Arbitrum</strong></td>
<td style="text-align: center;">Xueyan Tang et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.14773v1">2307.14773v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">This research aims to explore the security risks posed by compatibility and protocol differences in smart contract migration, using the migration of smart contracts from Ethereum to Arbitrum as a case study. Through literature review, online data collection, expert participation, and analysis of smart contract vulnerability cases, this paper conducts an in-depth research of the differences between Ethereum and Arbitrum in areas such as Messaging, Block Properties, Contract Address Alias, and Gas Fees. The research findings indicate the presence of certain security issues during the migration process from Ethereum to Arbitrum, such as abnormal operation of the sequencer resulting in outdated off-chain data retrieval, time-based logical errors, failed permission checks, DOS attacks, and gas loss due to L1-to-L2 transaction failures. To address these security issues, this paper proposes corresponding solutions and recommendations to ensure the security and meet the requirements of the migration process. Additionally, this research emphasizes the continued attention and support for the security issues of smart contract migration through the case of smart contract migration from Ethereum to Arbitrum. It is worth noting that this research is the first in-depth research of smart contract security migration from Ethereum to Arbitrum.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-27</strong></td>
<td style="text-align: center;"><strong>How to Train Your YouTube Recommender</strong></td>
<td style="text-align: center;">Alexander Liu et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.14551v1">2307.14551v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">YouTube provides features for users to indicate disinterest when presented with unwanted recommendations, such as the <code>Not interested'' and</code>Don\'t recommend channel'' buttons. These buttons are purported to allow the user to correct <code>mistakes'' made by the recommendation system. Yet, relatively little is known about the empirical efficacy of these buttons. Neither is much known about users' awareness of and confidence in them. To address these gaps, we simulated YouTube users with sock puppet agents. Each agent first executed a</code>stain phase'', where it watched many videos of one assigned topic; then it executed a <code>scrub phase'', where it tried to remove recommendations of the assigned topic. Each agent repeatedly applied a single scrubbing strategy, which included disliking previously-watched videos or deleting them from watch history, as well as clicking the</code>not interested'' or <code>don\'t recommend channel'' button on newly-recommended videos. Overall, we found that the stain phase significantly increased the fraction of the recommended videos on the user\'s homepage dedicated to the assigned topic. For the scrub phase, using the</code>Not interested'' button worked best, significantly reducing such recommendations in all topics tested, on average removing 88\% of them. Neither the stain phase nor the scrub phase, however, had much effect on videopage recommendations (those given to users while they watch a video). We also ran a survey ($N$ =300) asking adult YouTube users in the US whether they were aware of and used these buttons before, as well as how effective they found these buttons to be. We found that 44\% of participants were not aware that the ``Not interested'' button existed. However, those who were aware of this button often used it to remove unwanted recommendations (82.8\%) and found it to be modestly effective (3.42 out of 5).</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-26</strong></td>
<td style="text-align: center;"><strong>Event-based Vision for Early Prediction of Manipulation Actions</strong></td>
<td style="text-align: center;">Daniel Deniz et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.14332v1">2307.14332v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Neuromorphic visual sensors are artificial retinas that output sequences of asynchronous events when brightness changes occur in the scene. These sensors offer many advantages including very high temporal resolution, no motion blur and smart data compression ideal for real-time processing. In this study, we introduce an event-based dataset on fine-grained manipulation actions and perform an experimental study on the use of transformers for action prediction with events. There is enormous interest in the fields of cognitive robotics and human-robot interaction on understanding and predicting human actions as early as possible. Early prediction allows anticipating complex stages for planning, enabling effective and real-time interaction. Our Transformer network uses events to predict manipulation actions as they occur, using online inference. The model succeeds at predicting actions early on, building up confidence over time and achieving state-of-the-art classification. Moreover, the attention-based transformer architecture allows us to study the role of the spatio-temporal patterns selected by the model. Our experiments show that the Transformer network captures action dynamic features outperforming video-based approaches and succeeding with scenarios where the differences between actions lie in very subtle cues. Finally, we release the new event dataset, which is the first in the literature for manipulation action recognition. Code will be available at https://github.com/DaniDeniz/EventVisionTransformer.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-26</strong></td>
<td style="text-align: center;"><strong>Simulation of Open Quantum Systems via Low-Depth Convex Unitary Evolutions</strong></td>
<td style="text-align: center;">Joseph Peetz et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.14325v2">2307.14325v2</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Simulating physical systems on quantum devices is one of the most promising applications of quantum technology. Current quantum approaches to simulating open quantum systems are still practically challenging on NISQ-era devices, because they typically require ancilla qubits and extensive controlled sequences. In this work, we propose a hybrid quantum-classical approach for simulating a class of open system dynamics called random-unitary channels. These channels naturally decompose into a series of convex unitary evolutions, which can then be efficiently sampled and run as independent circuits. The method does not require deep ancilla frameworks and thus can be implemented with lower noise costs. We implement simulations of open quantum systems up to dozens of qubits and with large channel rank.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-26</strong></td>
<td style="text-align: center;"><strong>Large-scale Fully-Unsupervised Re-Identification</strong></td>
<td style="text-align: center;">Gabriel Bertocco et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.14278v1">2307.14278v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Fully-unsupervised Person and Vehicle Re-Identification have received increasing attention due to their broad applicability in surveillance, forensics, event understanding, and smart cities, without requiring any manual annotation. However, most of the prior art has been evaluated in datasets that have just a couple thousand samples. Such small-data setups often allow the use of costly techniques in time and memory footprints, such as Re-Ranking, to improve clustering results. Moreover, some previous work even pre-selects the best clustering hyper-parameters for each dataset, which is unrealistic in a large-scale fully-unsupervised scenario. In this context, this work tackles a more realistic scenario and proposes two strategies to learn from large-scale unlabeled data. The first strategy performs a local neighborhood sampling to reduce the dataset size in each iteration without violating neighborhood relationships. A second strategy leverages a novel Re-Ranking technique, which has a lower time upper bound complexity and reduces the memory complexity from O(n^2) to O(kn) with k &lt;&lt; n. To avoid the pre-selection of specific hyper-parameter values for the clustering algorithm, we also present a novel scheduling algorithm that adjusts the density parameter during training, to leverage the diversity of samples and keep the learning robust to noisy labeling. Finally, due to the complementary knowledge learned by different models, we also introduce a co-training strategy that relies upon the permutation of predicted pseudo-labels, among the backbones, with no need for any hyper-parameters or weighting optimization. The proposed methodology outperforms the state-of-the-art methods in well-known benchmarks and in the challenging large-scale Veri-Wild dataset, with a faster and memory-efficient Re-Ranking strategy, and a large-scale, noisy-robust, and ensemble-based learning approach.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-25</strong></td>
<td style="text-align: center;"><strong>Integration of Digital Twin and Federated Learning for Securing Vehicular Internet of Things</strong></td>
<td style="text-align: center;">Deepti Gupta et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.13794v1">2307.13794v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">In the present era of advanced technology, the Internet of Things (IoT) plays a crucial role in enabling smart connected environments. This includes various domains such as smart homes, smart healthcare, smart cities, smart vehicles, and many others.With ubiquitous smart connected devices and systems, a large amount of data associated with them is at a prime risk from malicious entities (e.g., users, devices, applications) in these systems. Innovative technologies, including cloud computing, Machine Learning (ML), and data analytics, support the development of anomaly detection models for the Vehicular Internet of Things (V-IoT), which encompasses collaborative automatic driving and enhanced transportation systems. However, traditional centralized anomaly detection models fail to provide better services for connected vehicles due to issues such as high latency, privacy leakage, performance overhead, and model drift. Recently, Federated Learning (FL) has gained significant recognition for its ability to address data privacy concerns in the IoT domain. Digital Twin (DT), proves beneficial in addressing uncertain crises and data security issues by creating a virtual replica that simulates various factors, including traffic trajectories, city policies, and vehicle utilization. However, the effectiveness of a V-IoT DT system heavily relies on the collection of long-term and high-quality data to make appropriate decisions. This paper introduces a Hierarchical Federated Learning (HFL) based anomaly detection model for V-IoT, aiming to enhance the accuracy of the model. Our proposed model integrates both DT and HFL approaches to create a comprehensive system for detecting malicious activities using an anomaly detection model. Additionally, real-world V-IoT use case scenarios are presented to demonstrate the application of the proposed model.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-25</strong></td>
<td style="text-align: center;"><strong>ChildGAN: Large Scale Synthetic Child Facial Data Using Domain Adaptation in StyleGAN</strong></td>
<td style="text-align: center;">Muhammad Ali Farooq et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.13746v1">2307.13746v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">In this research work, we proposed a novel ChildGAN, a pair of GAN networks for generating synthetic boys and girls facial data derived from StyleGAN2. ChildGAN is built by performing smooth domain transfer using transfer learning. It provides photo-realistic, high-quality data samples. A large-scale dataset is rendered with a variety of smart facial transformations: facial expressions, age progression, eye blink effects, head pose, skin and hair color variations, and variable lighting conditions. The dataset comprises more than 300k distinct data samples. Further, the uniqueness and characteristics of the rendered facial features are validated by running different computer vision application tests which include CNN-based child gender classifier, face localization and facial landmarks detection test, identity similarity evaluation using ArcFace, and lastly running eye detection and eye aspect ratio tests. The results demonstrate that synthetic child facial data of high quality offers an alternative to the cost and complexity of collecting a large-scale dataset from real children.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-25</strong></td>
<td style="text-align: center;"><strong>Insights into Cognitive Engagement: Comparing the Effectiveness of Game-Based and Video-Based Learning</strong></td>
<td style="text-align: center;">Shayla Sharmin et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.13637v1">2307.13637v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">The analysis of brain signals holds considerable importance in enhancing our comprehension of diverse learning techniques and cognitive mechanisms. Game-based learning is increasingly being recognized for its interactive and engaging educational approach. A pilot study of twelve participants divided into experimental and control groups was conducted to understand its effects on cognitive processes. Both groups were provided with the same contents regarding the basic structure of the graph. The participants in the experimental group engaged in a quiz-based game, while those in the control group watched a pre-recorded video. Functional Near-Infrared Spectroscopy (fNIRS) was employed to acquire cerebral signals, and a series of pre and post-tests were administered. The findings of our study indicate that the group engaged in the game activity displayed elevated levels of oxygenated hemoglobin compared to the group involved in watching videos. Conversely, the deoxygenated hemoglobin levels remained relatively consistent across both groups throughout the learning process. The aforementioned findings suggest that the use of game-based learning has a substantial influence on cognitive processes. Furthermore, it is evident that both the game and video groups exhibited higher neural activity in the Lateral Prefrontal cortex (PFC). The oxygenated hemoglobin ratio demonstrates that the game group had 2.33 times more neural processing in the Lateral PFC than the video group. This data is further supported by the knowledge gain analysis, which indicates that the game-based approach resulted in a 47.74% higher knowledge gain than the video group, as calculated from the difference in pre-and post-test scores.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-25</strong></td>
<td style="text-align: center;"><strong>On-Device Speaker Anonymization of Acoustic Embeddings for ASR based onFlexible Location Gradient Reversal Layer</strong></td>
<td style="text-align: center;">Md Asif Jalal et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.13343v1">2307.13343v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Smart devices serviced by large-scale AI models necessitates user data transfer to the cloud for inference. For speech applications, this means transferring private user information, e.g., speaker identity. Our paper proposes a privacy-enhancing framework that targets speaker identity anonymization while preserving speech recognition accuracy for our downstream task~-~Automatic Speech Recognition (ASR). The proposed framework attaches flexible gradient reversal based speaker adversarial layers to target layers within an ASR model, where speaker adversarial training anonymizes acoustic embeddings generated by the targeted layers to remove speaker identity. We propose on-device deployment by execution of initial layers of the ASR model, and transmitting anonymized embeddings to the cloud, where the rest of the model is executed while preserving privacy. Experimental results show that our method efficiently reduces speaker recognition relative accuracy by 33%, and improves ASR performance by achieving 6.2% relative Word Error Rate (WER) reduction.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-25</strong></td>
<td style="text-align: center;"><strong>A Pairwise Dataset for GUI Conversion and Retrieval between Android Phones and Tablets</strong></td>
<td style="text-align: center;">Han Hu et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.13225v1">2307.13225v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">With the popularity of smartphones and tablets, users have become accustomed to using different devices for different tasks, such as using their phones to play games and tablets to watch movies. To conquer the market, one app is often available on both smartphones and tablets. However, although one app has similar graphic user interfaces (GUIs) and functionalities on phone and tablet, current app developers typically start from scratch when developing a tablet-compatible version of their app, which drives up development costs and wastes existing design resources. Researchers are attempting to employ deep learning in automated GUIs development to enhance developers' productivity. Deep learning models rely heavily on high-quality datasets. There are currently several publicly accessible GUI page datasets for phones, but none for pairwise GUIs between phones and tablets. This poses a significant barrier to the employment of deep learning in automated GUI development. In this paper, we collect and make public the Papt dataset, which is a pairwise dataset for GUI conversion and retrieval between Android phones and tablets. The dataset contains 10,035 phone-tablet GUI page pairs from 5,593 phone-tablet app pairs. We illustrate the approaches of collecting pairwise data and statistical analysis of this dataset. We also illustrate the advantages of our dataset compared to other current datasets. Through preliminary experiments on this dataset, we analyse the present challenges of utilising deep learning in automated GUI development and find that our dataset can assist the application of some deep learning models to tasks involving automatic GUI development.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-24</strong></td>
<td style="text-align: center;"><strong>Evaluating the reliability of automatically generated pedestrian and bicycle crash surrogates</strong></td>
<td style="text-align: center;">Agnimitra Sengupta et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.13178v1">2307.13178v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Vulnerable road users (VRUs), such as pedestrians and bicyclists, are at a higher risk of being involved in crashes with motor vehicles, and crashes involving VRUs also are more likely to result in severe injuries or fatalities. Signalized intersections are a major safety concern for VRUs due to their complex and dynamic nature, highlighting the need to understand how these road users interact with motor vehicles and deploy evidence-based countermeasures to improve safety performance. Crashes involving VRUs are relatively infrequent, making it difficult to understand the underlying contributing factors. An alternative is to identify and use conflicts between VRUs and motorized vehicles as a surrogate for safety performance. Automatically detecting these conflicts using a video-based systems is a crucial step in developing smart infrastructure to enhance VRU safety. The Pennsylvania Department of Transportation conducted a study using video-based event monitoring system to assess VRU and motor vehicle interactions at fifteen signalized intersections across Pennsylvania to improve VRU safety performance. This research builds on that study to assess the reliability of automatically generated surrogates in predicting confirmed conflicts using advanced data-driven models. The surrogate data used for analysis include automatically collectable variables such as vehicular and VRU speeds, movements, post-encroachment time, in addition to manually collected variables like signal states, lighting, and weather conditions. The findings highlight the varying importance of specific surrogates in predicting true conflicts, some being more informative than others. The findings can assist transportation agencies to collect the right types of data to help prioritize infrastructure investments, such as bike lanes and crosswalks, and evaluate their effectiveness.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-24</strong></td>
<td style="text-align: center;"><strong>Schema-Driven Actionable Insight Generation and Smart Recommendation</strong></td>
<td style="text-align: center;">Allmin Susaiyah et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.13176v1">2307.13176v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">In natural language generation (NLG), insight mining is seen as a data-to-text task, where data is mined for interesting patterns and verbalised into 'insight' statements. An 'over-generate and rank' paradigm is intuitively used to generate such insights. The multidimensionality and subjectivity of this process make it challenging. This paper introduces a schema-driven method to generate actionable insights from data to drive growth and change. It also introduces a technique to rank the insights to align with user interests based on their feedback. We show preliminary qualitative results of the insights generated using our technique and demonstrate its ability to adapt to feedback.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-24</strong></td>
<td style="text-align: center;"><strong>A Connection between One-Step Regularization and Critic Regularization in Reinforcement Learning</strong></td>
<td style="text-align: center;">Benjamin Eysenbach et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.12968v1">2307.12968v1</a></td>
<td style="text-align: center;"><a href="https://github.com/ben-eysenbach/ac-connection">link</a></td>
<td style="text-align: center;">As with any machine learning problem with limited data, effective offline RL algorithms require careful regularization to avoid overfitting. One-step methods perform regularization by doing just a single step of policy improvement, while critic regularization methods do many steps of policy improvement with a regularized objective. These methods appear distinct. One-step methods, such as advantage-weighted regression and conditional behavioral cloning, truncate policy iteration after just one step. This ``early stopping'' makes one-step RL simple and stable, but can limit its asymptotic performance. Critic regularization typically requires more compute but has appealing lower-bound guarantees. In this paper, we draw a close connection between these methods: applying a multi-step critic regularization method with a regularization coefficient of 1 yields the same policy as one-step RL. While practical implementations violate our assumptions and critic regularization is typically applied with smaller regularization coefficients, our experiments nevertheless show that our analysis makes accurate, testable predictions about practical offline RL methods (CQL and one-step RL) with commonly-used hyperparameters. Our results that every problem can be solved with a single step of policy improvement, but rather that one-step RL might be competitive with critic regularization on RL problems that demand strong regularization.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-24</strong></td>
<td style="text-align: center;"><strong>Rechargeable Li/Cl$_2$ battery down to -80 °C</strong></td>
<td style="text-align: center;">Peng Liang et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.12947v1">2307.12947v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Low temperature rechargeable batteries are important to life in cold climates, polar/deep-sea expeditions and space explorations. Here, we report ~ 3.5 - 4 V rechargeable lithium/chlorine (Li/Cl2) batteries operating down to -80 {\deg}C, employing Li metal negative electrode, a novel CO2 activated porous carbon (KJCO2) as the positive electrode, and a high ionic conductivity (~ 5 to 20 mS cm-1 from -80 {\deg}C to 25 {\deg}C) electrolyte comprised of 1 M aluminum chloride (AlCl3), 0.95 M lithium chloride (LiCl), and 0.05 M lithium bis(fluorosulfonyl)imide (LiFSI) in low melting point (-104.5 {\deg}C) thionyl chloride (SOCl2). Between room-temperature and -80 {\deg}C, the Li/Cl2 battery delivered up to ~ 30,000 - 4,500 mAh g-1 first discharge capacity and a 1,200 - 5,000 mAh g-1 reversible capacity (discharge voltages in ~ 3.5 to 3.1 V) over up to 130 charge-discharge cycles. Mass spectrometry and X-ray photoelectron spectroscopy (XPS) probed Cl2 trapped in the porous carbon upon LiCl electro-oxidation during charging. At lower temperature down to -80 {\deg}C, SCl2/S2Cl2 and Cl2 generated by electro-oxidation in the charging step were trapped in porous KJCO2 carbon, allowing for reversible reduction to afford a high discharge voltage plateau near ~ 4 V with up to ~ 1000 mAh g-1 capacity for SCl2/S2Cl2 reduction and up to ~ 4000 mAh g-1 capacity at ~ 3.1 V plateau for Cl2 reduction. Towards practical use, we made CR2032 Li/Cl2 battery cells to drive digital watches at -40 {\deg}C and light emitting diode at -80 {\deg}C, opening Li/Cl2 secondary batteries for ultra-cold conditions.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-24</strong></td>
<td style="text-align: center;"><strong>Economic Analysis of Smart Roadside Infrastructure Sensors for Connected and Automated Mobility</strong></td>
<td style="text-align: center;">Laurent Kloeker et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.12893v1">2307.12893v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Smart roadside infrastructure sensors in the form of intelligent transportation system stations (ITS-Ss) are increasingly deployed worldwide at relevant traffic nodes. The resulting digital twins of the real environment are suitable for developing and validating connected and automated driving functions and for increasing the operational safety of intelligent vehicles by providing ITS-S real-time data. However, ITS-Ss are very costly to establish and operate. The choice of sensor technology also has an impact on the overall costs as well as on the data quality. So far, there is only insufficient knowledge about the concrete expenses that arise with the construction of different ITS-S setups. Within this work, multiple modular infrastructure sensor setups are investigated with the help of a life cycle cost analysis (LCCA). Their economic efficiency, different user requirements and sensor data qualities are considered. Based on the static cost model, a Monte Carlo simulation is performed, to generate a range of possible project costs and to quantify the financial risks of implementing ITS-S projects of different scales. Due to its modularity, the calculation model is suitable for diverse applications and outputs a distinctive evaluation of the underlying cost-benefit ratio of investigated setups.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-24</strong></td>
<td style="text-align: center;"><strong>Multi-Shooting Differential Dynamic Programming for Hybrid Systems using Analytical Derivatives</strong></td>
<td style="text-align: center;">Shubham Singh et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.12606v1">2307.12606v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Differential Dynamic Programming (DDP) is a popular technique used to generate motion for dynamic-legged robots in the recent past. However, in most cases, only the first-order partial derivatives of the underlying dynamics are used, resulting in the iLQR approach. Neglecting the second-order terms often slows down the convergence rate compared to full DDP. Multi-Shooting is another popular technique to improve robustness, especially if the dynamics are highly non-linear. In this work, we consider Multi-Shooting DDP for trajectory optimization of a bounding gait for a simplified quadruped model. As the main contribution, we develop Second-Order analytical partial derivatives of the rigid-body contact dynamics, extending our previous results for fixed/floating base models with multi-DoF joints. Finally, we show the benefits of a novel Quasi-Newton method for approximating second-order derivatives of the dynamics, leading to order-of-magnitude speedups in the convergence compared to the full DDP method.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-24</strong></td>
<td style="text-align: center;"><strong>Automated Mapping of Adaptive App GUIs from Phones to TVs</strong></td>
<td style="text-align: center;">Han Hu et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.12522v1">2307.12522v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">With the increasing interconnection of smart devices, users often desire to adopt the same app on quite different devices for identical tasks, such as watching the same movies on both their smartphones and TV.   However, the significant differences in screen size, aspect ratio, and interaction styles make it challenging to adapt Graphical User Interfaces (GUIs) across these devices.   Although there are millions of apps available on Google Play, only a few thousand are designed to support smart TV displays.   Existing techniques to map a mobile app GUI to a TV either adopt a responsive design, which struggles to bridge the substantial gap between phone and TV or use mirror apps for improved video display, which requires hardware support and extra engineering efforts.   Instead of developing another app for supporting TVs, we propose a semi-automated approach to generate corresponding adaptive TV GUIs, given the phone GUIs as the input.   Based on our empirical study of GUI pairs for TV and phone in existing apps, we synthesize a list of rules for grouping and classifying phone GUIs, converting them to TV GUIs, and generating dynamic TV layouts and source code for the TV display.   Our tool is not only beneficial to developers but also to GUI designers, who can further customize the generated GUIs for their TV app development.   An evaluation and user study demonstrate the accuracy of our generated GUIs and the usefulness of our tool.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-23</strong></td>
<td style="text-align: center;"><strong>An Efficient Authentication Protocol for Smart Grid Communication Based on On-Chip-Error-Correcting Physical Unclonable Function</strong></td>
<td style="text-align: center;">Masoud Kaveh et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.12374v1">2307.12374v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Security has become a main concern for the smart grid to move from research and development to industry. The concept of security has usually referred to resistance to threats by an active or passive attacker. However, since smart meters (SMs) are often placed in unprotected areas, physical security has become one of the important security goals in the smart grid. Physical unclonable functions (PUFs) have been largely utilized for ensuring physical security in recent years, though their reliability has remained a major problem to be practically used in cryptographic applications. Although fuzzy extractors have been considered as a solution to solve the reliability problem of PUFs, they put a considerable computational cost to the resource-constrained SMs. To that end, we first propose an on-chip-error-correcting (OCEC) PUF that efficiently generates stable digits for the authentication process. Afterward, we introduce a lightweight authentication protocol between the SMs and neighborhood gateway (NG) based on the proposed PUF. The provable security analysis shows that not only the proposed protocol can stand secure in the Canetti-Krawczyk (CK) adversary model but also provides additional security features. Also, the performance evaluation demonstrates the significant improvement of the proposed scheme in comparison with the state-of-the-art.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-23</strong></td>
<td style="text-align: center;"><strong>Semantic Communication-Empowered Traffic Management using Vehicle Count Prediction</strong></td>
<td style="text-align: center;">Sachin Kadam et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.12254v1">2307.12254v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Vehicle count prediction is an important aspect of smart city traffic management. Most major roads are monitored by cameras with computing and transmitting capabilities. These cameras provide data to the central traffic controller (CTC), which is in charge of traffic control management. In this paper, we propose a joint CNN-LSTM-based semantic communication (SemCom) model in which the semantic encoder of a camera extracts the relevant semantics from raw images. The encoded semantics are then sent to the CTC by the transmitter in the form of symbols. The semantic decoder of the CTC predicts the vehicle count on each road based on the sequence of received symbols and develops a traffic management strategy accordingly. An optimization problem to improve the quality of experience (QoE) is introduced and numerically solved, taking into account constraints such as vehicle user safety, transmit power of camera devices, vehicle count prediction accuracy, and semantic entropy. Using numerical results, we show that the proposed SemCom model reduces overhead by $54.42\%$ when compared to source encoder/decoder methods. Also, we demonstrate through simulations that the proposed model outperforms state-of-the-art models in terms of mean absolute error (MAE) and QoE.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-23</strong></td>
<td style="text-align: center;"><strong>LiveRetro: Visual Analytics for Strategic Retrospect in Livestream E-Commerce</strong></td>
<td style="text-align: center;">Yuchen Wu et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.12213v1">2307.12213v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Livestream e-commerce integrates live streaming and online shopping, allowing viewers to make purchases while watching. However, effective marketing strategies remain a challenge due to limited empirical research and subjective biases from the absence of quantitative data. Current tools fail to capture the interdependence between live performances and feedback. This study identified computational features, formulated design requirements, and developed LiveRetro, an interactive visual analytics system. It enables comprehensive retrospective analysis of livestream e-commerce for streamers, viewers, and merchandise. LiveRetro employs enhanced visualization and time-series forecasting models to align performance features and feedback, identifying influences at channel, merchandise, feature, and segment levels. Through case studies and expert interviews, the system provides deep insights into the relationship between live performance and streaming statistics, enabling efficient strategic analysis from multiple perspectives.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-23</strong></td>
<td style="text-align: center;"><strong>Content Censorship in the InterPlanetary File System</strong></td>
<td style="text-align: center;">Srivatsan Sridhar et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.12212v1">2307.12212v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">The InterPlanetary File System (IPFS) is currently the largest decentralized storage solution in operation, with thousands of active participants and millions of daily content transfers. IPFS is used as remote data storage for numerous blockchain-based smart contracts, Non-Fungible Tokens (NFT), and decentralized applications.   We present a content censorship attack that can be executed with minimal effort and cost, and that prevents the retrieval of any chosen content in the IPFS network. The attack exploits a conceptual issue in a core component of IPFS, the Kademlia Distributed Hash Table (DHT), which is used to resolve content IDs to peer addresses. We provide efficient detection and mitigation mechanisms for this vulnerability. Our mechanisms achieve a 99.6\% detection rate and mitigate 100\% of the detected attacks with minimal signaling and computational overhead. We followed responsible disclosure procedures, and our countermeasures are scheduled for deployment in the future versions of IPFS.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-22</strong></td>
<td style="text-align: center;"><strong>AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities</strong></td>
<td style="text-align: center;">Victor Adewopo et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.12128v1">2307.12128v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Accident detection and traffic analysis is a critical component of smart city and autonomous transportation systems that can reduce accident frequency, severity and improve overall traffic management. This paper presents a comprehensive analysis of traffic accidents in different regions across the United States using data from the National Highway Traffic Safety Administration (NHTSA) Crash Report Sampling System (CRSS). To address the challenges of accident detection and traffic analysis, this paper proposes a framework that uses traffic surveillance cameras and action recognition systems to detect and respond to traffic accidents spontaneously. Integrating the proposed framework with emergency services will harness the power of traffic cameras and machine learning algorithms to create an efficient solution for responding to traffic accidents and reducing human errors. Advanced intelligence technologies, such as the proposed accident detection systems in smart cities, will improve traffic management and traffic accident severity. Overall, this study provides valuable insights into traffic accidents in the US and presents a practical solution to enhance the safety and efficiency of transportation systems.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-22</strong></td>
<td style="text-align: center;"><strong>Blockchain-based Cloud Data Deduplication Scheme with Fair Incentives</strong></td>
<td style="text-align: center;">Mallikarjun Reddy Dorsala et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.12052v1">2307.12052v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">With the rapid development of cloud computing, vast amounts of duplicated data are being uploaded to the cloud, wasting storage resources. Deduplication (dedup) is an efficient solution to save storage costs of cloud storage providers (CSPs) by storing only one copy of the uploaded data. However, cloud users do not benefit directly from dedup and may be reluctant to dedup their data. To motivate the cloud users towards dedup, CSPs offer incentives on storage fees. The problems with the existing dedup schemes are that they do not consider: (1) correctness - the incentive offered to a cloud user should be computed correctly without any prejudice. (2) fairness - the cloud user receives the file link and access rights of the uploaded data if and only if the CSP receives the storage fee. Meeting these requirements without a trusted party is non-trivial, and most of the existing dedup schemes do not apply. Another drawback is that most of the existing schemes emphasize incentives to cloud users but failed to provide a reliable incentive mechanism.   As public Blockchain networks emulate the properties of trusted parties, in this paper, we propose a new Blockchain-based dedup scheme to meet the above requirements. In our scheme, a smart contract computes the incentives on storage fee, and the fairness rules are encoded into the smart contract for facilitating fair payments between the CSPs and cloud users. We prove the correctness and fairness of the proposed scheme. We also design a new incentive mechanism and show that the scheme is individually rational and incentive compatible. Furthermore, we conduct experiments by implementing the designed smart contract on Ethereum local Blockchain network and list the transactional and financial costs of interacting with the designed smart contract.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-22</strong></td>
<td style="text-align: center;"><strong>Two-stream Multi-level Dynamic Point Transformer for Two-person Interaction Recognition</strong></td>
<td style="text-align: center;">Yao Liu et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.11973v1">2307.11973v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">As a fundamental aspect of human life, two-person interactions contain meaningful information about people's activities, relationships, and social settings. Human action recognition serves as the foundation for many smart applications, with a strong focus on personal privacy. However, recognizing two-person interactions poses more challenges due to increased body occlusion and overlap compared to single-person actions. In this paper, we propose a point cloud-based network named Two-stream Multi-level Dynamic Point Transformer for two-person interaction recognition. Our model addresses the challenge of recognizing two-person interactions by incorporating local-region spatial information, appearance information, and motion information. To achieve this, we introduce a designed frame selection method named Interval Frame Sampling (IFS), which efficiently samples frames from videos, capturing more discriminative information in a relatively short processing time. Subsequently, a frame features learning module and a two-stream multi-level feature aggregation module extract global and partial features from the sampled frames, effectively representing the local-region spatial information, appearance information, and motion information related to the interactions. Finally, we apply a transformer to perform self-attention on the learned features for the final classification. Extensive experiments are conducted on two large-scale datasets, the interaction subsets of NTU RGB+D 60 and NTU RGB+D 120. The results show that our network outperforms state-of-the-art approaches across all standard evaluation settings.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-21</strong></td>
<td style="text-align: center;"><strong>Building3D: An Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds</strong></td>
<td style="text-align: center;">Ruisheng Wang et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.11914v1">2307.11914v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">Urban modeling from LiDAR point clouds is an important topic in computer vision, computer graphics, photogrammetry and remote sensing. 3D city models have found a wide range of applications in smart cities, autonomous navigation, urban planning and mapping etc. However, existing datasets for 3D modeling mainly focus on common objects such as furniture or cars. Lack of building datasets has become a major obstacle for applying deep learning technology to specific domains such as urban modeling. In this paper, we present a urban-scale dataset consisting of more than 160 thousands buildings along with corresponding point clouds, mesh and wire-frame models, covering 16 cities in Estonia about 998 Km2. We extensively evaluate performance of state-of-the-art algorithms including handcrafted and deep feature based methods. Experimental results indicate that Building3D has challenges of high intra-class variance, data imbalance and large-scale noises. The Building3D is the first and largest urban-scale building modeling benchmark, allowing a comparison of supervised and self-supervised learning methods. We believe that our Building3D will facilitate future research on urban modeling, aerial path planning, mesh simplification, and semantic/part segmentation etc.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-21</strong></td>
<td style="text-align: center;"><strong>Selecting the motion ground truth for loose-fitting wearables: benchmarking optical MoCap methods</strong></td>
<td style="text-align: center;">Lala Shakti Swarup Ray et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.11881v2">2307.11881v2</a></td>
<td style="text-align: center;"><a href="https://github.com/lalasray/dmcb">link</a></td>
<td style="text-align: center;">To help smart wearable researchers choose the optimal ground truth methods for motion capturing (MoCap) for all types of loose garments, we present a benchmark, DrapeMoCapBench (DMCB), specifically designed to evaluate the performance of optical marker-based and marker-less MoCap. High-cost marker-based MoCap systems are well-known as precise golden standards. However, a less well-known caveat is that they require skin-tight fitting markers on bony areas to ensure the specified precision, making them questionable for loose garments. On the other hand, marker-less MoCap methods powered by computer vision models have matured over the years, which have meager costs as smartphone cameras would suffice. To this end, DMCB uses large real-world recorded MoCap datasets to perform parallel 3D physics simulations with a wide range of diversities: six levels of drape from skin-tight to extremely draped garments, three levels of motions and six body type - gender combinations to benchmark state-of-the-art optical marker-based and marker-less MoCap methods to identify the best-performing method in different scenarios. In assessing the performance of marker-based and low-cost marker-less MoCap for casual loose garments both approaches exhibit significant performance loss (&gt;10cm), but for everyday activities involving basic and fast motions, marker-less MoCap slightly outperforms marker-based MoCap, making it a favorable and cost-effective choice for wearable studies.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-21</strong></td>
<td style="text-align: center;"><strong>Smart Machine Vision for Universal Spatial Mode Reconstruction</strong></td>
<td style="text-align: center;">José D. Huerta-Morales et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.11841v1">2307.11841v1</a></td>
<td style="text-align: center;"><a href="https://github.com/mquirozj/Smart-camera-for-spatial-mode-reconstruction">link</a></td>
<td style="text-align: center;">Structured light beams, in particular those carrying orbital angular momentum (OAM), have gained a lot of attention due to their potential for enlarging the transmission capabilities of communication systems. However, the use of OAM-carrying light in communications faces two major problems, namely distortions introduced during propagation in disordered media, such as the atmosphere or optical fibers, and the large divergence that high-order OAM modes experience. While the use of non-orthogonal modes may offer a way to circumvent the divergence of high-order OAM fields, artificial intelligence (AI) algorithms have shown promise for solving the mode-distortion issue. Unfortunately, current AI-based algorithms make use of large-amount data-handling protocols that generally lead to large processing time and high power consumption. Here we show that a low-power, low-cost image sensor can itself act as an artificial neural network that simultaneously detects and reconstructs distorted OAM-carrying beams. We demonstrate the capabilities of our device by reconstructing (with a 95$\%$ efficiency) individual Vortex, Laguerre-Gaussian (LG) and Bessel modes, as well as hybrid (non-orthogonal) coherent superpositions of such modes. Our work provides a potentially useful basis for the development of low-power-consumption, light-based communication devices.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-21</strong></td>
<td style="text-align: center;"><strong>Zero-touch realization of Pervasive Artificial Intelligence-as-a-service in 6G networks</strong></td>
<td style="text-align: center;">Emna Baccour et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.11468v1">2307.11468v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">The vision of the upcoming 6G technologies, characterized by ultra-dense network, low latency, and fast data rate is to support Pervasive AI (PAI) using zero-touch solutions enabling self-X (e.g., self-configuration, self-monitoring, and self-healing) services. However, the research on 6G is still in its infancy, and only the first steps have been taken to conceptualize its design, investigate its implementation, and plan for use cases. Toward this end, academia and industry communities have gradually shifted from theoretical studies of AI distribution to real-world deployment and standardization. Still, designing an end-to-end framework that systematizes the AI distribution by allowing easier access to the service using a third-party application assisted by a zero-touch service provisioning has not been well explored. In this context, we introduce a novel platform architecture to deploy a zero-touch PAI-as-a-Service (PAIaaS) in 6G networks supported by a blockchain-based smart system. This platform aims to standardize the pervasive AI at all levels of the architecture and unify the interfaces in order to facilitate the service deployment across application and infrastructure domains, relieve the users worries about cost, security, and resource allocation, and at the same time, respect the 6G stringent performance requirements. As a proof of concept, we present a Federated Learning-as-a-service use case where we evaluate the ability of our proposed system to self-optimize and self-adapt to the dynamics of 6G networks in addition to minimizing the users' perceived costs.</td>
</tr>
<tr>
<td style="text-align: center;"><strong>2023-07-21</strong></td>
<td style="text-align: center;"><strong>Large Language Model-based System to Provide Immediate Feedback to Students in Flipped Classroom Preparation Learning</strong></td>
<td style="text-align: center;">Shintaro Uchiyama et.al.</td>
<td style="text-align: center;"><a href="http://arxiv.org/abs/2307.11388v1">2307.11388v1</a></td>
<td style="text-align: center;">null</td>
<td style="text-align: center;">This paper proposes a system that uses large language models to provide immediate feedback to students in flipped classroom preparation learning. This study aimed to solve challenges in the flipped classroom model, such as ensuring that students are emotionally engaged and motivated to learn. Students often have questions about the content of lecture videos in the preparation of flipped classrooms, but it is difficult for teachers to answer them immediately. The proposed system was developed using the ChatGPT API on a video-watching support system for preparation learning that is being used in real practice. Answers from ChatGPT often do not align with the context of the student's question. Therefore, this paper also proposes a method to align the answer with the context. This paper also proposes a method to collect the teacher's answers to the students' questions and use them as additional guides for the students. This paper discusses the design and implementation of the proposed system.</td>
</tr>
</tbody>
</table>


  




                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.indexes"], "search": "../../assets/javascripts/workers/search.74e28a9f.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.220ee61c.min.js"></script>
      
    
  </body>
</html>